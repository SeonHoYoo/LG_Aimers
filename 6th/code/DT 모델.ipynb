{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c36ad7-412a-486f-92fb-35cd4bf5b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Decision Tree 최적의 하이퍼파라미터: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Accuracy: 0.6585\n",
      "ROC-AUC: 0.7117\n",
      "Confusion Matrix:\n",
      "[[10794  8828]\n",
      " [ 4574 15047]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 2. 다운샘플링: 클래스 불균형 해소\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 3. 피처/타겟 분리: 제외할 칼럼 제거\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 4. 학습/검증 데이터 분리 (Stratify 옵션 사용)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Decision Tree 모델 하이퍼파라미터 튜닝 (GridSearchCV)\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"Decision Tree 최적의 하이퍼파라미터:\", grid_search.best_params_)\n",
    "\n",
    "# 6. 검증 데이터 평가\n",
    "y_pred = best_dt.predict(X_val)\n",
    "y_prob = best_dt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_prob)\n",
    "conf_mat = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {:.4f}\".format(roc_auc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0176fb84-e0d2-4cd1-ab3a-2a85cce7f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9720 candidates, totalling 48600 fits\n",
      "Decision Tree 최적의 하이퍼파라미터: {'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Accuracy: 0.6606\n",
      "ROC-AUC: 0.7144\n",
      "Confusion Matrix:\n",
      "[[ 9707  9915]\n",
      " [ 3404 16217]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 2. 다운샘플링: 클래스 불균형 해소\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 3. 피처/타겟 분리: 제외할 칼럼 제거\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 4. 학습/검증 데이터 분리 (Stratify 옵션 사용)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Decision Tree 모델 하이퍼파라미터 튜닝 (깊이 있는 탐색)\n",
    "param_grid = {\n",
    "    'criterion': ['entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [15, 20],\n",
    "    'min_samples_split': [20, 40],\n",
    "    'min_samples_leaf': [4, 8, 16],\n",
    "    'max_features': [None, 'sqrt'],\n",
    "    'max_leaf_nodes': [50],\n",
    "    'min_impurity_decrease': [0.0, 0.001, 0.01],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"Decision Tree 최적의 하이퍼파라미터:\", grid_search.best_params_)\n",
    "\n",
    "# 6. 검증 데이터 평가\n",
    "y_pred = best_dt.predict(X_val)\n",
    "y_prob = best_dt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_prob)\n",
    "conf_mat = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {:.4f}\".format(roc_auc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0673f39b-0933-4b56-96b2-2a789b0af7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Decision Tree 최적의 하이퍼파라미터: {'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 15, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 16, 'min_samples_split': 20, 'splitter': 'best'}\n",
      "Accuracy: 0.6606\n",
      "ROC-AUC: 0.7144\n",
      "Confusion Matrix:\n",
      "[[ 9707  9915]\n",
      " [ 3404 16217]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 2. 다운샘플링: 클래스 불균형 해소\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 3. 피처/타겟 분리: 제외할 칼럼 제거\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 4. 학습/검증 데이터 분리 (Stratify 옵션 사용)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Decision Tree 모델 하이퍼파라미터 튜닝 (깊이 있는 탐색)\n",
    "param_grid = {\n",
    "    'criterion': ['entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [15, 20],\n",
    "    'min_samples_split': [20, 40],\n",
    "    'min_samples_leaf': [4, 8, 16],\n",
    "    'max_features': [None, 'sqrt'],\n",
    "    'max_leaf_nodes': [50],\n",
    "    'min_impurity_decrease': [0.0, 0.001, 0.01],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"Decision Tree 최적의 하이퍼파라미터:\", grid_search.best_params_)\n",
    "\n",
    "# 6. 검증 데이터 평가\n",
    "y_pred = best_dt.predict(X_val)\n",
    "y_prob = best_dt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_prob)\n",
    "conf_mat = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {:.4f}\".format(roc_auc))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd7463-3cb1-4b0e-8fd2-1d9af72e93e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "122f1cca-760b-4508-8995-69193ffa3c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- Decision Tree 최적의 하이퍼파라미터 -----\n",
      "{'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 15, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 16, 'min_samples_split': 20, 'splitter': 'best'}\n",
      "Decision Tree Accuracy: 0.6606\n",
      "Decision Tree ROC-AUC: 0.7144\n",
      "Decision Tree Confusion Matrix:\n",
      "[[ 9707  9915]\n",
      " [ 3404 16217]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- RandomForest 최적의 하이퍼파라미터 -----\n",
      "{'max_depth': 200, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "RandomForest Accuracy: 0.6626\n",
      "RandomForest ROC-AUC: 0.7166\n",
      "RandomForest Confusion Matrix:\n",
      "[[10598  9024]\n",
      " [ 4216 15405]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 300, 'num_leaves': 31}\n",
      "LightGBM (기본) Accuracy: 0.6636\n",
      "LightGBM (기본) ROC-AUC: 0.7189\n",
      "LightGBM (기본) Confusion Matrix:\n",
      "[[10463  9159]\n",
      " [ 4043 15578]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 500, 'num_leaves': 70}\n",
      "LightGBM (튜닝) Accuracy: 0.6632\n",
      "LightGBM (튜닝) ROC-AUC: 0.7182\n",
      "LightGBM (튜닝) Confusion Matrix:\n",
      "[[10500  9122]\n",
      " [ 4095 15526]]\n",
      "============================================\n",
      "\n",
      "----- Ensemble 결과 (4개 모델 평균) -----\n",
      "Ensemble Accuracy: 0.6627\n",
      "Ensemble ROC-AUC: 0.7184\n",
      "Ensemble Confusion Matrix:\n",
      "[[10425  9197]\n",
      " [ 4039 15582]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#####################################\n",
    "# 1. 데이터 불러오기 및 전처리\n",
    "#####################################\n",
    "# 학습/테스트 데이터 로드 (cp949 인코딩)\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 다운샘플링: 클래스 불균형 해소 (임신 성공 여부 0 vs 1)\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 사용할 칼럼 외 제거 (불필요한 정보 배제)\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 학습/검증 데이터 분리 (stratify로 클래스 비율 유지)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#####################################\n",
    "# 2. Model 1: Decision Tree (깊은 하이퍼파라미터 탐색)\n",
    "#####################################\n",
    "param_grid_dt = {\n",
    "    'criterion': ['entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [15],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [16],\n",
    "    'max_features': [None],\n",
    "    'max_leaf_nodes': [50],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'ccp_alpha': [0.0]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "print(\"----- Decision Tree 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_dt.best_params_)\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_val)\n",
    "y_prob_dt = best_dt.predict_proba(X_val)[:, 1]\n",
    "acc_dt = accuracy_score(y_val, y_pred_dt)\n",
    "roc_dt = roc_auc_score(y_val, y_prob_dt)\n",
    "conf_dt = confusion_matrix(y_val, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy: {:.4f}\".format(acc_dt))\n",
    "print(\"Decision Tree ROC-AUC: {:.4f}\".format(roc_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(conf_dt)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 3. Model 2: RandomForest (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [200, 300],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [8]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"----- RandomForest 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_val)\n",
    "y_prob_rf = best_rf.predict_proba(X_val)[:, 1]\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "roc_rf = roc_auc_score(y_val, y_prob_rf)\n",
    "conf_rf = confusion_matrix(y_val, y_pred_rf)\n",
    "print(\"RandomForest Accuracy: {:.4f}\".format(acc_rf))\n",
    "print(\"RandomForest ROC-AUC: {:.4f}\".format(roc_rf))\n",
    "print(\"RandomForest Confusion Matrix:\")\n",
    "print(conf_rf)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 4. Model 3: LightGBM (기본 모델, GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb_2 = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [-1],\n",
    "    'n_estimators': [300],\n",
    "    'num_leaves': [31]\n",
    "}\n",
    "lgb_model_plain = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb_plain = GridSearchCV(\n",
    "    estimator=lgb_model_plain,\n",
    "    param_grid=param_grid_lgb_2,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb_plain.fit(X_train, y_train)\n",
    "best_lgb_plain = grid_search_lgb_plain.best_estimator_\n",
    "print(\"----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb_plain.best_params_)\n",
    "\n",
    "y_pred_lgb_plain = best_lgb_plain.predict(X_val)\n",
    "y_prob_lgb_plain = best_lgb_plain.predict_proba(X_val)[:, 1]\n",
    "acc_lgb_plain = accuracy_score(y_val, y_pred_lgb_plain)\n",
    "roc_lgb_plain = roc_auc_score(y_val, y_prob_lgb_plain)\n",
    "conf_lgb_plain = confusion_matrix(y_val, y_pred_lgb_plain)\n",
    "print(\"LightGBM (기본) Accuracy: {:.4f}\".format(acc_lgb_plain))\n",
    "print(\"LightGBM (기본) ROC-AUC: {:.4f}\".format(roc_lgb_plain))\n",
    "print(\"LightGBM (기본) Confusion Matrix:\")\n",
    "print(conf_lgb_plain)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 5. Model 4: LightGBM (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [500],\n",
    "    'learning_rate': [0.01],\n",
    "    'num_leaves': [70],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "lgb_estimator = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    estimator=lgb_estimator,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "best_lgb = grid_search_lgb.best_estimator_\n",
    "print(\"----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb.best_params_)\n",
    "\n",
    "y_pred_lgb = best_lgb.predict(X_val)\n",
    "y_prob_lgb = best_lgb.predict_proba(X_val)[:, 1]\n",
    "acc_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "roc_lgb = roc_auc_score(y_val, y_prob_lgb)\n",
    "conf_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "print(\"LightGBM (튜닝) Accuracy: {:.4f}\".format(acc_lgb))\n",
    "print(\"LightGBM (튜닝) ROC-AUC: {:.4f}\".format(roc_lgb))\n",
    "print(\"LightGBM (튜닝) Confusion Matrix:\")\n",
    "print(conf_lgb)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 6. Ensemble: 4개 모델의 예측 확률 평균 기반 보팅\n",
    "#####################################\n",
    "# 각 모델의 양성(임신 성공) 클래스에 대한 예측 확률\n",
    "ensemble_prob = (y_prob_dt + y_prob_rf + y_prob_lgb_plain + y_prob_lgb) / 4\n",
    "ensemble_pred = (ensemble_prob >= 0.5).astype(int)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_val, ensemble_pred)\n",
    "ensemble_roc = roc_auc_score(y_val, ensemble_prob)\n",
    "ensemble_conf = confusion_matrix(y_val, ensemble_pred)\n",
    "\n",
    "print(\"----- Ensemble 결과 (4개 모델 평균) -----\")\n",
    "print(\"Ensemble Accuracy: {:.4f}\".format(ensemble_acc))\n",
    "print(\"Ensemble ROC-AUC: {:.4f}\".format(ensemble_roc))\n",
    "print(\"Ensemble Confusion Matrix:\")\n",
    "print(ensemble_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc97b548-6513-43eb-ae40-5a6b3796a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- Decision Tree 최적의 하이퍼파라미터 -----\n",
      "{'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 15, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 16, 'min_samples_split': 20, 'splitter': 'best'}\n",
      "Decision Tree Accuracy: 0.6606\n",
      "Decision Tree ROC-AUC: 0.7144\n",
      "Decision Tree Confusion Matrix:\n",
      "[[ 9707  9915]\n",
      " [ 3404 16217]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- RandomForest 최적의 하이퍼파라미터 -----\n",
      "{'max_depth': 200, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "RandomForest Accuracy: 0.6626\n",
      "RandomForest ROC-AUC: 0.7166\n",
      "RandomForest Confusion Matrix:\n",
      "[[10598  9024]\n",
      " [ 4216 15405]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 300, 'num_leaves': 31}\n",
      "LightGBM (기본) Accuracy: 0.6636\n",
      "LightGBM (기본) ROC-AUC: 0.7189\n",
      "LightGBM (기본) Confusion Matrix:\n",
      "[[10463  9159]\n",
      " [ 4043 15578]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 500, 'num_leaves': 70}\n",
      "LightGBM (튜닝) Accuracy: 0.6632\n",
      "LightGBM (튜닝) ROC-AUC: 0.7182\n",
      "LightGBM (튜닝) Confusion Matrix:\n",
      "[[10500  9122]\n",
      " [ 4095 15526]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----- Logistic Regression 최적의 하이퍼파라미터 -----\n",
      "{'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Logistic Regression Accuracy: 0.6404\n",
      "Logistic Regression ROC-AUC: 0.6961\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[10749  8873]\n",
      " [ 5240 14381]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoyo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:12:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- XGBoost 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "XGBoost Accuracy: 0.6625\n",
      "XGBoost ROC-AUC: 0.7186\n",
      "XGBoost Confusion Matrix:\n",
      "[[10573  9049]\n",
      " [ 4195 15426]]\n",
      "============================================\n",
      "\n",
      "----- Ensemble 결과 (6개 모델 평균) -----\n",
      "Ensemble Accuracy: 0.6651\n",
      "Ensemble ROC-AUC: 0.7193\n",
      "Ensemble Confusion Matrix:\n",
      "[[10413  9209]\n",
      " [ 3935 15686]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#####################################\n",
    "# 1. 데이터 불러오기 및 전처리\n",
    "#####################################\n",
    "# 학습/테스트 데이터 로드 (cp949 인코딩)\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 다운샘플링: 클래스 불균형 해소 (임신 성공 여부 0 vs 1)\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 사용할 칼럼 외 제거 (불필요한 정보 배제)\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 학습/검증 데이터 분리 (stratify로 클래스 비율 유지)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#####################################\n",
    "# 2. Model 1: Decision Tree (깊은 하이퍼파라미터 탐색)\n",
    "#####################################\n",
    "param_grid_dt = {\n",
    "    'criterion': ['entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [15],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [16],\n",
    "    'max_features': [None],\n",
    "    'max_leaf_nodes': [50],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'ccp_alpha': [0.0]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "print(\"----- Decision Tree 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_dt.best_params_)\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_val)\n",
    "y_prob_dt = best_dt.predict_proba(X_val)[:, 1]\n",
    "acc_dt = accuracy_score(y_val, y_pred_dt)\n",
    "roc_dt = roc_auc_score(y_val, y_prob_dt)\n",
    "conf_dt = confusion_matrix(y_val, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy: {:.4f}\".format(acc_dt))\n",
    "print(\"Decision Tree ROC-AUC: {:.4f}\".format(roc_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(conf_dt)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 3. Model 2: RandomForest (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [200, 300],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [8]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"----- RandomForest 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_val)\n",
    "y_prob_rf = best_rf.predict_proba(X_val)[:, 1]\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "roc_rf = roc_auc_score(y_val, y_prob_rf)\n",
    "conf_rf = confusion_matrix(y_val, y_pred_rf)\n",
    "print(\"RandomForest Accuracy: {:.4f}\".format(acc_rf))\n",
    "print(\"RandomForest ROC-AUC: {:.4f}\".format(roc_rf))\n",
    "print(\"RandomForest Confusion Matrix:\")\n",
    "print(conf_rf)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 4. Model 3: LightGBM (기본 모델, GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb_2 = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [-1],\n",
    "    'n_estimators': [300],\n",
    "    'num_leaves': [31]\n",
    "}\n",
    "lgb_model_plain = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb_plain = GridSearchCV(\n",
    "    estimator=lgb_model_plain,\n",
    "    param_grid=param_grid_lgb_2,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb_plain.fit(X_train, y_train)\n",
    "best_lgb_plain = grid_search_lgb_plain.best_estimator_\n",
    "print(\"----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb_plain.best_params_)\n",
    "\n",
    "y_pred_lgb_plain = best_lgb_plain.predict(X_val)\n",
    "y_prob_lgb_plain = best_lgb_plain.predict_proba(X_val)[:, 1]\n",
    "acc_lgb_plain = accuracy_score(y_val, y_pred_lgb_plain)\n",
    "roc_lgb_plain = roc_auc_score(y_val, y_prob_lgb_plain)\n",
    "conf_lgb_plain = confusion_matrix(y_val, y_pred_lgb_plain)\n",
    "print(\"LightGBM (기본) Accuracy: {:.4f}\".format(acc_lgb_plain))\n",
    "print(\"LightGBM (기본) ROC-AUC: {:.4f}\".format(roc_lgb_plain))\n",
    "print(\"LightGBM (기본) Confusion Matrix:\")\n",
    "print(conf_lgb_plain)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 5. Model 4: LightGBM (튜닝 모델, GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [500],\n",
    "    'learning_rate': [0.01],\n",
    "    'num_leaves': [70],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "lgb_estimator = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    estimator=lgb_estimator,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "best_lgb = grid_search_lgb.best_estimator_\n",
    "print(\"----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb.best_params_)\n",
    "\n",
    "y_pred_lgb = best_lgb.predict(X_val)\n",
    "y_prob_lgb = best_lgb.predict_proba(X_val)[:, 1]\n",
    "acc_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "roc_lgb = roc_auc_score(y_val, y_prob_lgb)\n",
    "conf_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "print(\"LightGBM (튜닝) Accuracy: {:.4f}\".format(acc_lgb))\n",
    "print(\"LightGBM (튜닝) ROC-AUC: {:.4f}\".format(roc_lgb))\n",
    "print(\"LightGBM (튜닝) Confusion Matrix:\")\n",
    "print(conf_lgb)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 6. Model 5: Logistic Regression (GridSearchCV 활용)\n",
    "#####################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "lr = LogisticRegression(random_state=42)\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "print(\"----- Logistic Regression 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lr.best_params_)\n",
    "\n",
    "y_pred_lr = best_lr.predict(X_val)\n",
    "y_prob_lr = best_lr.predict_proba(X_val)[:, 1]\n",
    "acc_lr = accuracy_score(y_val, y_pred_lr)\n",
    "roc_lr = roc_auc_score(y_val, y_prob_lr)\n",
    "conf_lr = confusion_matrix(y_val, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy: {:.4f}\".format(acc_lr))\n",
    "print(\"Logistic Regression ROC-AUC: {:.4f}\".format(roc_lr))\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(conf_lr)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 7. Model 6: XGBoost (GridSearchCV 활용)\n",
    "#####################################\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "print(\"----- XGBoost 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_val)\n",
    "y_prob_xgb = best_xgb.predict_proba(X_val)[:, 1]\n",
    "acc_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "roc_xgb = roc_auc_score(y_val, y_prob_xgb)\n",
    "conf_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy: {:.4f}\".format(acc_xgb))\n",
    "print(\"XGBoost ROC-AUC: {:.4f}\".format(roc_xgb))\n",
    "print(\"XGBoost Confusion Matrix:\")\n",
    "print(conf_xgb)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 8. Ensemble: 6개 모델의 예측 확률 평균 기반 보팅\n",
    "#####################################\n",
    "# 각 모델의 양성(임신 성공) 클래스에 대한 예측 확률\n",
    "ensemble_prob = (y_prob_dt + y_prob_rf + y_prob_lgb_plain + y_prob_lgb + y_prob_lr + y_prob_xgb) / 6\n",
    "ensemble_pred = (ensemble_prob >= 0.5).astype(int)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_val, ensemble_pred)\n",
    "ensemble_roc = roc_auc_score(y_val, ensemble_prob)\n",
    "ensemble_conf = confusion_matrix(y_val, ensemble_pred)\n",
    "\n",
    "print(\"----- Ensemble 결과 (6개 모델 평균) -----\")\n",
    "print(\"Ensemble Accuracy: {:.4f}\".format(ensemble_acc))\n",
    "print(\"Ensemble ROC-AUC: {:.4f}\".format(ensemble_roc))\n",
    "print(\"Ensemble Confusion Matrix:\")\n",
    "print(ensemble_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb92df09-865c-418d-b67d-71919fbb2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- Decision Tree 최적의 하이퍼파라미터 -----\n",
      "{'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': 15, 'max_features': None, 'max_leaf_nodes': 50, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 16, 'min_samples_split': 20, 'splitter': 'best'}\n",
      "Decision Tree Accuracy: 0.6606\n",
      "Decision Tree ROC-AUC: 0.7144\n",
      "Decision Tree Confusion Matrix:\n",
      "[[ 9707  9915]\n",
      " [ 3404 16217]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----- RandomForest 최적의 하이퍼파라미터 -----\n",
      "{'max_depth': 200, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 500}\n",
      "RandomForest Accuracy: 0.6626\n",
      "RandomForest ROC-AUC: 0.7166\n",
      "RandomForest Confusion Matrix:\n",
      "[[10598  9024]\n",
      " [ 4216 15405]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 300, 'num_leaves': 31}\n",
      "LightGBM (기본) Accuracy: 0.6636\n",
      "LightGBM (기본) ROC-AUC: 0.7189\n",
      "LightGBM (기본) Confusion Matrix:\n",
      "[[10463  9159]\n",
      " [ 4043 15578]]\n",
      "============================================\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 45783, number of negative: 45782\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 160\n",
      "[LightGBM] [Info] Number of data points in the train set: 91565, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000022\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 500, 'num_leaves': 70}\n",
      "LightGBM (튜닝) Accuracy: 0.6632\n",
      "LightGBM (튜닝) ROC-AUC: 0.7182\n",
      "LightGBM (튜닝) Confusion Matrix:\n",
      "[[10500  9122]\n",
      " [ 4095 15526]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----- Logistic Regression 최적의 하이퍼파라미터 -----\n",
      "{'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Logistic Regression Accuracy: 0.6404\n",
      "Logistic Regression ROC-AUC: 0.6961\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[10749  8873]\n",
      " [ 5240 14381]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoyo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:12:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- XGBoost 최적의 하이퍼파라미터 -----\n",
      "{'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "XGBoost Accuracy: 0.6625\n",
      "XGBoost ROC-AUC: 0.7186\n",
      "XGBoost Confusion Matrix:\n",
      "[[10573  9049]\n",
      " [ 4195 15426]]\n",
      "============================================\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "----- ExtraTrees 최적의 하이퍼파라미터 -----\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "ExtraTrees Accuracy: 0.6605\n",
      "ExtraTrees ROC-AUC: 0.7145\n",
      "ExtraTrees Confusion Matrix:\n",
      "[[10308  9314]\n",
      " [ 4010 15611]]\n",
      "============================================\n",
      "\n",
      "----- Ensemble 결과 (7개 모델 평균) -----\n",
      "Ensemble Accuracy: 0.6654\n",
      "Ensemble ROC-AUC: 0.7193\n",
      "Ensemble Confusion Matrix:\n",
      "[[10428  9194]\n",
      " [ 3935 15686]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#####################################\n",
    "# 1. 데이터 불러오기 및 전처리\n",
    "#####################################\n",
    "# 학습/테스트 데이터 로드 (cp949 인코딩)\n",
    "train_data = pd.read_csv('train2.csv', encoding='cp949')\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 다운샘플링: 클래스 불균형 해소 (임신 성공 여부 0 vs 1)\n",
    "df_majority = train_data[train_data['임신 성공 여부'] == 0]\n",
    "df_minority = train_data[train_data['임신 성공 여부'] == 1]\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,\n",
    "    n_samples=len(df_minority),\n",
    "    random_state=42\n",
    ")\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# 사용할 칼럼 외 제거 (불필요한 정보 배제)\n",
    "cols_to_drop = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X = df_downsampled.drop(columns=cols_to_drop)\n",
    "y = df_downsampled['임신 성공 여부']\n",
    "\n",
    "# 학습/검증 데이터 분리 (stratify로 클래스 비율 유지)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#####################################\n",
    "# 2. Model 1: Decision Tree (깊은 하이퍼파라미터 탐색)\n",
    "#####################################\n",
    "param_grid_dt = {\n",
    "    'criterion': ['entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [15],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [16],\n",
    "    'max_features': [None],\n",
    "    'max_leaf_nodes': [50],\n",
    "    'min_impurity_decrease': [0.0],\n",
    "    'ccp_alpha': [0.0]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "print(\"----- Decision Tree 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_dt.best_params_)\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_val)\n",
    "y_prob_dt = best_dt.predict_proba(X_val)[:, 1]\n",
    "acc_dt = accuracy_score(y_val, y_pred_dt)\n",
    "roc_dt = roc_auc_score(y_val, y_prob_dt)\n",
    "conf_dt = confusion_matrix(y_val, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy: {:.4f}\".format(acc_dt))\n",
    "print(\"Decision Tree ROC-AUC: {:.4f}\".format(roc_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(conf_dt)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 3. Model 2: RandomForest (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [200, 300],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [8]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"----- RandomForest 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_val)\n",
    "y_prob_rf = best_rf.predict_proba(X_val)[:, 1]\n",
    "acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "roc_rf = roc_auc_score(y_val, y_prob_rf)\n",
    "conf_rf = confusion_matrix(y_val, y_pred_rf)\n",
    "print(\"RandomForest Accuracy: {:.4f}\".format(acc_rf))\n",
    "print(\"RandomForest ROC-AUC: {:.4f}\".format(roc_rf))\n",
    "print(\"RandomForest Confusion Matrix:\")\n",
    "print(conf_rf)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 4. Model 3: LightGBM (기본 모델, GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb_2 = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [-1],\n",
    "    'n_estimators': [300],\n",
    "    'num_leaves': [31]\n",
    "}\n",
    "lgb_model_plain = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb_plain = GridSearchCV(\n",
    "    estimator=lgb_model_plain,\n",
    "    param_grid=param_grid_lgb_2,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb_plain.fit(X_train, y_train)\n",
    "best_lgb_plain = grid_search_lgb_plain.best_estimator_\n",
    "print(\"----- LightGBM (기본 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb_plain.best_params_)\n",
    "\n",
    "y_pred_lgb_plain = best_lgb_plain.predict(X_val)\n",
    "y_prob_lgb_plain = best_lgb_plain.predict_proba(X_val)[:, 1]\n",
    "acc_lgb_plain = accuracy_score(y_val, y_pred_lgb_plain)\n",
    "roc_lgb_plain = roc_auc_score(y_val, y_prob_lgb_plain)\n",
    "conf_lgb_plain = confusion_matrix(y_val, y_pred_lgb_plain)\n",
    "print(\"LightGBM (기본) Accuracy: {:.4f}\".format(acc_lgb_plain))\n",
    "print(\"LightGBM (기본) ROC-AUC: {:.4f}\".format(roc_lgb_plain))\n",
    "print(\"LightGBM (기본) Confusion Matrix:\")\n",
    "print(conf_lgb_plain)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 5. Model 4: LightGBM (튜닝 모델, GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [500],\n",
    "    'learning_rate': [0.01],\n",
    "    'num_leaves': [70],\n",
    "    'max_depth': [20]\n",
    "}\n",
    "lgb_estimator = lgb.LGBMClassifier(random_state=42)\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    estimator=lgb_estimator,\n",
    "    param_grid=param_grid_lgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "best_lgb = grid_search_lgb.best_estimator_\n",
    "print(\"----- LightGBM (튜닝 모델) 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lgb.best_params_)\n",
    "\n",
    "y_pred_lgb = best_lgb.predict(X_val)\n",
    "y_prob_lgb = best_lgb.predict_proba(X_val)[:, 1]\n",
    "acc_lgb = accuracy_score(y_val, y_pred_lgb)\n",
    "roc_lgb = roc_auc_score(y_val, y_prob_lgb)\n",
    "conf_lgb = confusion_matrix(y_val, y_pred_lgb)\n",
    "print(\"LightGBM (튜닝) Accuracy: {:.4f}\".format(acc_lgb))\n",
    "print(\"LightGBM (튜닝) ROC-AUC: {:.4f}\".format(roc_lgb))\n",
    "print(\"LightGBM (튜닝) Confusion Matrix:\")\n",
    "print(conf_lgb)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 6. Model 5: Logistic Regression (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "lr = LogisticRegression(random_state=42)\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "print(\"----- Logistic Regression 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_lr.best_params_)\n",
    "\n",
    "y_pred_lr = best_lr.predict(X_val)\n",
    "y_prob_lr = best_lr.predict_proba(X_val)[:, 1]\n",
    "acc_lr = accuracy_score(y_val, y_pred_lr)\n",
    "roc_lr = roc_auc_score(y_val, y_prob_lr)\n",
    "conf_lr = confusion_matrix(y_val, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy: {:.4f}\".format(acc_lr))\n",
    "print(\"Logistic Regression ROC-AUC: {:.4f}\".format(roc_lr))\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(conf_lr)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 7. Model 6: XGBoost (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [0.01],\n",
    "    'subsample': [0.8]\n",
    "}\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "print(\"----- XGBoost 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_val)\n",
    "y_prob_xgb = best_xgb.predict_proba(X_val)[:, 1]\n",
    "acc_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "roc_xgb = roc_auc_score(y_val, y_prob_xgb)\n",
    "conf_xgb = confusion_matrix(y_val, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy: {:.4f}\".format(acc_xgb))\n",
    "print(\"XGBoost ROC-AUC: {:.4f}\".format(roc_xgb))\n",
    "print(\"XGBoost Confusion Matrix:\")\n",
    "print(conf_xgb)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#####################################\n",
    "# 8. Model 7: ExtraTreesClassifier (GridSearchCV 활용)\n",
    "#####################################\n",
    "param_grid_et = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [2]\n",
    "}\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "grid_search_et = GridSearchCV(\n",
    "    estimator=et,\n",
    "    param_grid=param_grid_et,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_et.fit(X_train, y_train)\n",
    "best_et = grid_search_et.best_estimator_\n",
    "print(\"----- ExtraTrees 최적의 하이퍼파라미터 -----\")\n",
    "print(grid_search_et.best_params_)\n",
    "\n",
    "y_pred_et = best_et.predict(X_val)\n",
    "y_prob_et = best_et.predict_proba(X_val)[:, 1]\n",
    "acc_et = accuracy_score(y_val, y_pred_et)\n",
    "roc_et = roc_auc_score(y_val, y_prob_et)\n",
    "conf_et = confusion_matrix(y_val, y_pred_et)\n",
    "print(\"ExtraTrees Accuracy: {:.4f}\".format(acc_et))\n",
    "print(\"ExtraTrees ROC-AUC: {:.4f}\".format(roc_et))\n",
    "print(\"ExtraTrees Confusion Matrix:\")\n",
    "print(conf_et)\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "\n",
    "#####################################\n",
    "# 11. Ensemble: 9개 모델의 예측 확률 평균 기반 보팅\n",
    "#####################################\n",
    "ensemble_prob = (y_prob_dt + y_prob_rf + y_prob_lgb_plain + y_prob_lgb + y_prob_lr + \n",
    "                 y_prob_xgb + y_prob_et) / 7\n",
    "ensemble_pred = (ensemble_prob >= 0.5).astype(int)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_val, ensemble_pred)\n",
    "ensemble_roc = roc_auc_score(y_val, ensemble_prob)\n",
    "ensemble_conf = confusion_matrix(y_val, ensemble_pred)\n",
    "\n",
    "print(\"----- Ensemble 결과 (7개 모델 평균) -----\")\n",
    "print(\"Ensemble Accuracy: {:.4f}\".format(ensemble_acc))\n",
    "print(\"Ensemble ROC-AUC: {:.4f}\".format(ensemble_roc))\n",
    "print(\"Ensemble Confusion Matrix:\")\n",
    "print(ensemble_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9e59b6-238c-4c23-abde-d37df8be95ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  임신 성공 여부 예측  임신 성공 여부 예측_확률\n",
      "0  TEST_00000            0        0.048366\n",
      "1  TEST_00001            0        0.048560\n",
      "2  TEST_00002            0        0.421340\n",
      "3  TEST_00003            0        0.270972\n",
      "4  TEST_00004            1        0.630890\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 2. 테스트 데이터 예측\n",
    "####################################\n",
    "# 테스트 데이터 불러오기 (cp949 인코딩)\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 테스트 데이터 전처리: 학습 시 사용했던 제외할 칼럼과 동일하게 제거\n",
    "cols_to_drop_test = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                     '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                     'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                     '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X_test = test_data.drop(columns=cols_to_drop_test)\n",
    "\n",
    "# 각 모델의 예측 확률 계산\n",
    "# (모든 모델은 학습 시 GridSearchCV를 통해 최적화된 최적 모델입니다.)\n",
    "test_prob_dt      = best_dt.predict_proba(X_test)[:, 1]\n",
    "test_prob_rf      = best_rf.predict_proba(X_test)[:, 1]\n",
    "test_prob_lgb_plain = best_lgb_plain.predict_proba(X_test)[:, 1]\n",
    "test_prob_lgb     = best_lgb.predict_proba(X_test)[:, 1]\n",
    "test_prob_lr      = best_lr.predict_proba(X_test)[:, 1]\n",
    "test_prob_xgb     = best_xgb.predict_proba(X_test)[:, 1]\n",
    "test_prob_et      = best_et.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 7개 모델의 예측 확률 평균 계산 (Ensemble)\n",
    "ensemble_test_prob = (test_prob_dt + test_prob_rf + test_prob_lgb_plain +\n",
    "                      test_prob_lgb + test_prob_lr + test_prob_xgb + test_prob_et) / 7\n",
    "\n",
    "# 평균 확률이 0.5 이상이면 1, 미만이면 0으로 최종 예측\n",
    "ensemble_test_pred = (ensemble_test_prob >= 0.5).astype(int)\n",
    "\n",
    "# 예측 결과를 테스트 데이터에 추가\n",
    "test_data['임신 성공 여부 예측'] = ensemble_test_pred\n",
    "test_data['임신 성공 여부 예측_확률'] = ensemble_test_prob\n",
    "\n",
    "# 결과 확인\n",
    "print(test_data[['ID', '임신 성공 여부 예측', '임신 성공 여부 예측_확률']].head())\n",
    "\n",
    "# 예측 결과를 CSV 파일로 저장\n",
    "test_data.to_csv('test_ivf_ensemble_predictions_0225_2734.csv', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "923832a0-7788-4648-881f-e636bbc664d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stacking Ensemble 결과 -----\n",
      "Stacking Ensemble Accuracy: 0.6638\n",
      "Stacking Ensemble ROC-AUC: 0.7174\n",
      "Stacking Ensemble Confusion Matrix:\n",
      "[[10527  9095]\n",
      " [ 4097 15524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -------------------------------\n",
    "# 이미 학습된 개별 모델(best_dt, best_rf, best_lgb_plain, best_lgb, best_lr, best_xgb, best_et)\n",
    "# 이 모델들은 이전 코드에서 GridSearchCV를 통해 최적화된 모델들입니다.\n",
    "# -------------------------------\n",
    "\n",
    "# 스태킹 앙상블 베이스 모델로 사용할 모델 리스트 구성\n",
    "estimators = [\n",
    "    ('dt', best_dt),\n",
    "    ('rf', best_rf),\n",
    "    ('lgb_plain', best_lgb_plain),\n",
    "    ('lgb_tuned', best_lgb),\n",
    "    ('lr', best_lr),\n",
    "    ('xgb', best_xgb),\n",
    "    ('et', best_et)\n",
    "]\n",
    "\n",
    "# 최종 메타 모델 (예시: Logistic Regression)\n",
    "meta_estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# StackingClassifier 구성 (passthrough=True 옵션은 원본 특성도 메타 모델에 함께 전달합니다)\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_estimator,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# 스태킹 앙상블 학습\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대한 예측\n",
    "y_pred_stack = stacking_clf.predict(X_val)\n",
    "y_prob_stack = stacking_clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 평가 지표 계산\n",
    "acc_stack = accuracy_score(y_val, y_pred_stack)\n",
    "roc_stack = roc_auc_score(y_val, y_prob_stack)\n",
    "conf_stack = confusion_matrix(y_val, y_pred_stack)\n",
    "\n",
    "print(\"----- Stacking Ensemble 결과 -----\")\n",
    "print(\"Stacking Ensemble Accuracy: {:.4f}\".format(acc_stack))\n",
    "print(\"Stacking Ensemble ROC-AUC: {:.4f}\".format(roc_stack))\n",
    "print(\"Stacking Ensemble Confusion Matrix:\")\n",
    "print(conf_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56bb97c5-4b93-4b6d-8f27-da26eddd2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  임신 성공 여부 예측  임신 성공 여부 예측_확률\n",
      "0  TEST_00000            0        0.029528\n",
      "1  TEST_00001            0        0.036881\n",
      "2  TEST_00002            0        0.374338\n",
      "3  TEST_00003            0        0.229708\n",
      "4  TEST_00004            1        0.648581\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# 2. 테스트 데이터 예측 (Stacking Ensemble 사용)\n",
    "####################################\n",
    "# 테스트 데이터 불러오기 (cp949 인코딩)\n",
    "test_data = pd.read_csv('test2.csv', encoding='cp949')\n",
    "\n",
    "# 테스트 데이터 전처리: 학습 시 사용했던 제외할 칼럼과 동일하게 제거\n",
    "cols_to_drop_test = ['ID', '시술 시기 코드', '시술 유형', '임신 성공 여부',\n",
    "                     '여성 주 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인',\n",
    "                     'IVF 시술 횟수', 'IVF 출산 횟수', '혼합된 난자 수',\n",
    "                     '동결 배아 사용 여부', '신선 배아 사용 여부']\n",
    "X_test = test_data.drop(columns=cols_to_drop_test)\n",
    "\n",
    "# StackingClassifier를 사용한 테스트 데이터 예측\n",
    "# (stacking_clf는 이전에 베이스 모델들과 메타 모델을 사용해 학습된 StackingClassifier입니다.)\n",
    "y_test_pred = stacking_clf.predict(X_test)\n",
    "y_test_prob = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 예측 결과를 테스트 데이터에 추가\n",
    "test_data['임신 성공 여부 예측'] = y_test_pred\n",
    "test_data['임신 성공 여부 예측_확률'] = y_test_prob\n",
    "\n",
    "# 결과 확인\n",
    "print(test_data[['ID', '임신 성공 여부 예측', '임신 성공 여부 예측_확률']].head())\n",
    "\n",
    "# 예측 결과를 CSV 파일로 저장\n",
    "test_data.to_csv('test_ivf_ensemble_predictions_0225_stacking.csv', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c46390-aea0-4bb4-9120-d5bd5c1d900b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
